# SPDX-FileCopyrightText: Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# Quick inference test using TRT-LLM PyTorch backend with received weights
apiVersion: v1
kind: Pod
metadata:
  name: trtllm-inference-test
  labels:
    app: trtllm-inference-test
spec:
  restartPolicy: Never
  containers:
    - name: inference
      # TRT-LLM release container with full PyTorch backend support
      image: nvcr.io/nvidia/tensorrt-llm/release:0.21.0
      imagePullPolicy: Always
      securityContext:
        capabilities:
          add:
            - IPC_LOCK
      env:
        - name: MODEL_PATH
          value: "/models"
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token-secret
              key: HF_TOKEN
              optional: true
      command: ["/bin/bash", "-c"]
      args:
        - |
          set -ex
          
          echo "=== TRT-LLM Inference Test ==="
          echo "Model path: $MODEL_PATH"
          
          # Check TRT-LLM version
          python3 -c "import tensorrt_llm; print(f'TRT-LLM version: {tensorrt_llm.__version__}')"
          
          # Write inference script to file (avoids MPI/heredoc issues)
          cat > /tmp/inference_test.py << 'SCRIPT_EOF'
          import os
          import time
          
          def main():
              model_path = os.environ.get("MODEL_PATH", "/models")
              print(f"Loading model from: {model_path}")
          
              try:
                  from tensorrt_llm import LLM, SamplingParams
                  
                  print("Initializing TRT-LLM with PyTorch backend...")
                  print("This uses the new PyTorch workflow - no engine build needed!")
                  t0 = time.time()
                  
                  # Use PyTorch backend for HuggingFace models (TRT-LLM 0.17+)
                  llm = LLM(
                      model=model_path,
                      tensor_parallel_size=8,
                      dtype="bfloat16",
                  )
                  
                  load_time = time.time() - t0
                  print(f"Model loaded in {load_time:.1f}s")
                  
                  # Test generation
                  prompts = ["Hello, I am a large language model"]
                  sampling_params = SamplingParams(temperature=0.7, max_tokens=50)
                  
                  print("Running inference...")
                  t0 = time.time()
                  outputs = llm.generate(prompts, sampling_params)
                  gen_time = time.time() - t0
                  
                  for output in outputs:
                      print(f"Prompt: {output.prompt}")
                      print(f"Generated: {output.outputs[0].text}")
                      tokens = len(output.outputs[0].token_ids)
                      print(f"Tokens: {tokens}, Time: {gen_time:.2f}s, TPS: {tokens/gen_time:.1f}")
                  
                  print("\n=== TRT-LLM PYTORCH BACKEND INFERENCE PASSED! ===")
                  
              except Exception as e:
                  print(f"TRT-LLM Error: {e}")
                  import traceback
                  traceback.print_exc()
                  
                  # Fallback: Try with transformers
                  print("\nFallback: Testing with transformers...")
                  try:
                      from transformers import AutoModelForCausalLM, AutoTokenizer
                      import torch
                      
                      tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                      model = AutoModelForCausalLM.from_pretrained(
                          model_path,
                          torch_dtype=torch.bfloat16,
                          device_map="auto",
                          trust_remote_code=True,
                      )
                      
                      inputs = tokenizer("Hello, I am", return_tensors="pt").to("cuda")
                      outputs = model.generate(**inputs, max_new_tokens=20)
                      print(f"Generated: {tokenizer.decode(outputs[0])}")
                      print("Transformers inference PASSED!")
                  except Exception as e2:
                      print(f"Transformers fallback also failed: {e2}")
          
          if __name__ == '__main__':
              main()
          SCRIPT_EOF
          
          # Run the inference script
          python3 /tmp/inference_test.py
          
          echo "=== Test Complete ==="
      resources:
        limits:
          nvidia.com/gpu: "8"
        requests:
          nvidia.com/gpu: "8"
          memory: "200Gi"
          cpu: "16"
      volumeMounts:
        - name: shm
          mountPath: /dev/shm
        - name: model-cache
          mountPath: /models

  volumes:
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 64Gi
    - name: model-cache
      persistentVolumeClaim:
        claimName: model-weights-storage
  imagePullSecrets:
    - name: nvcr-imagepullsecret
