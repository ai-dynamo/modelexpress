# TRT-LLM Target: Single container, NO PVC, receives weights + config via MX server
# Phase 1 test: Uses checkpoint_format="mx-p2p" TRT-LLM plugin
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trtllm-target
  labels:
    app: trtllm-target
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trtllm-target
  template:
    metadata:
      labels:
        app: trtllm-target
    spec:
      containers:
        - name: target
          image: nvcr.io/nvidian/dynamo-dev/modelexpress-trtllm-client:ph1-sysnixl
          imagePullPolicy: Always
          env:
            - name: MODEL_NAME
              value: "Qwen/Qwen2.5-0.5B"
            - name: MODEL_EXPRESS_URL
              value: "modelexpress-server:8001"
            - name: NIXL_LOG_LEVEL
              value: "INFO"
            - name: UCX_LOG_LEVEL
              value: "WARN"
            - name: UCX_TLS
              value: "rc_x,rc,dc_x,dc,cuda_copy"
            - name: UCX_RNDV_SCHEME
              value: "get_zcopy"
            - name: UCX_RNDV_THRESH
              value: "0"
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          command: ["python3", "-c"]
          args:
            - |
              import os, time, logging
              logging.basicConfig(level=logging.INFO, format='%(asctime)s %(name)s %(levelname)s %(message)s')
              logger = logging.getLogger("trtllm-target")

              model_name = os.environ["MODEL_NAME"]

              # ============================================================
              # Phase 1: Custom Checkpoint Loader (zero disk I/O)
              #
              # Uses MxCheckpointLoader registered as "mx-p2p":
              #   1. Config loaded from MX server via gRPC (no PVC needed)
              #   2. Weights received via NIXL RDMA (no disk I/O)
              #   3. System NIXL used (same UCX as MPI â€” no conflict)
              #   4. Coalescing disabled for system NIXL compatibility
              # ============================================================

              logger.info("Registering MxCheckpointLoader...")
              from modelexpress.trtllm_checkpoint_loader import _register_loader
              MxCheckpointLoader = _register_loader()
              loader = MxCheckpointLoader()
              logger.info("MxCheckpointLoader ready")

              logger.info("Creating TRT-LLM LLM with custom checkpoint loader...")
              from tensorrt_llm import LLM, SamplingParams

              t0 = time.perf_counter()
              llm = LLM(
                  model=model_name,
                  checkpoint_loader=loader,
                  tensor_parallel_size=1,
              )
              load_time = time.perf_counter() - t0
              logger.info(f"Model loaded in {load_time:.2f}s")

              # Test inference
              logger.info("Running inference test...")
              prompts = ["Hello, I am a large language model"]
              params = SamplingParams(max_tokens=50, temperature=0.7)
              outputs = llm.generate(prompts, params)

              for output in outputs:
                  text = output.outputs[0].text
                  logger.info(f"Generated: {text}")

              logger.info("=" * 60)
              logger.info(f"SUCCESS: TRT-LLM P2P Phase 1!")
              logger.info(f"Total load time: {load_time:.2f}s")
              logger.info("=" * 60)

              # Keep alive for inspection
              while True:
                  time.sleep(3600)
          resources:
            limits:
              nvidia.com/gpu: "1"
              rdma/ib: "1"
            requests:
              nvidia.com/gpu: "1"
              rdma/ib: "1"
              memory: "16Gi"
              cpu: "4"
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      imagePullSecrets:
        - name: nvcr-imagepullsecret
