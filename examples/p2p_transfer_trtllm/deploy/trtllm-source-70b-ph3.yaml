# Phase 3 Source: Llama 70B (TP=8) — loads model per-rank, publishes live GPU params
# Single process loads 8 TRT-LLM model shards sequentially (one per GPU),
# registers each rank's params with NIXL, publishes metadata to MX server.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trtllm-source-70b
  labels:
    app: trtllm-source-70b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trtllm-source-70b
  template:
    metadata:
      labels:
        app: trtllm-source-70b
    spec:
      containers:
        - name: source
          image: nvcr.io/nvidian/dynamo-dev/modelexpress-trtllm-client:ph3
          imagePullPolicy: Always
          env:
            - name: MODEL_NAME
              value: "meta-llama/Llama-3.1-70B-Instruct"
            - name: MODEL_EXPRESS_URL
              value: "modelexpress-server:8001"
            - name: HF_HUB_CACHE
              value: "/models"
            - name: TP_SIZE
              value: "8"
            - name: NIXL_LOG_LEVEL
              value: "INFO"
            - name: UCX_LOG_LEVEL
              value: "WARN"
            - name: UCX_TLS
              value: "rc_x,rc,dc_x,dc,cuda_copy"
            - name: UCX_RNDV_SCHEME
              value: "get_zcopy"
            - name: UCX_RNDV_THRESH
              value: "0"
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: HF_TOKEN
          command: ["bash", "-c"]
          args:
            - |
              cat > /tmp/source_tp8.py << 'PYEOF'
              import os, time, logging, torch
              from mpi4py import MPI

              comm = MPI.COMM_WORLD
              rank = comm.Get_rank()
              world_size = comm.Get_size()

              logging.basicConfig(level=logging.INFO, format=f'%(asctime)s [rank{rank}] %(name)s %(levelname)s %(message)s')
              logger = logging.getLogger("trtllm-live-source-70b")

              torch.cuda.set_device(rank)
              model_name = os.environ["MODEL_NAME"]
              mx_server = os.environ["MODEL_EXPRESS_URL"]

              # Rank 0 resolves model path, broadcasts to all ranks
              if rank == 0:
                  from huggingface_hub import snapshot_download
                  local_path = snapshot_download(model_name, cache_dir=os.environ.get("HF_HUB_CACHE", "/models"))
                  logger.info("Model resolved to: %s", local_path)
              else:
                  local_path = None
              local_path = comm.bcast(local_path, root=0)

              from tensorrt_llm._torch.models.modeling_auto import AutoModelForCausalLM
              from tensorrt_llm._torch.models.modeling_utils import MetaInitMode
              from tensorrt_llm._torch.models.checkpoints.hf.checkpoint_loader import HfCheckpointLoader
              from tensorrt_llm._torch.models.checkpoints.hf.config_loader import HfConfigLoader
              from tensorrt_llm.mapping import Mapping
              from modelexpress.trtllm_live_transfer import MxLiveSource

              t0 = time.perf_counter()
              mapping = Mapping(world_size=world_size, tp_size=world_size, rank=rank)
              logger.info("Loading model shard (rank %d/%d) on GPU %d", rank, world_size, rank)

              config = HfConfigLoader().load(local_path, mapping=mapping, trust_remote_code=True)

              with MetaInitMode():
                  model = AutoModelForCausalLM.from_config(config)

              def _init_meta(t, dev=rank):
                  if t.device == torch.device('meta'):
                      return torch.empty_like(t, device=f'cuda:{dev}')
                  return t

              model._apply(_init_meta)
              model.to(f'cuda:{rank}')

              ckpt_loader = HfCheckpointLoader()
              weights = ckpt_loader.load_weights(local_path, mapping=mapping)
              weight_mapper = ckpt_loader.get_initialized_weight_mapper(model, config)
              model.load_weights(weights, weight_mapper=weight_mapper)

              params = list(model.named_parameters())
              total_bytes = sum(p.numel() * p.element_size() for _, p in params)
              elapsed = time.perf_counter() - t0
              logger.info("Rank %d: %d params (%.2f GB) loaded in %.1fs", rank, len(params), total_bytes / 1e9, elapsed)

              # Publish this rank's params via NIXL
              source = MxLiveSource(model, model_name, mx_server)

              # Monkey-patch _collect_model_files to use the correct model path
              # (the default glob search picks up the wrong model when cache has multiple)
              def _collect_from_path():
                  model_files = {}
                  for fname in ["config.json", "tokenizer.json", "tokenizer_config.json",
                                "generation_config.json", "special_tokens_map.json"]:
                      fpath = os.path.join(local_path, fname)
                      if os.path.exists(fpath):
                          with open(fpath, "rb") as f:
                              model_files[fname] = f.read()
                  return model_files
              source._collect_model_files = _collect_from_path

              source.publish()

              comm.Barrier()
              if rank == 0:
                  total_elapsed = time.perf_counter() - t0
                  logger.info("All %d ranks published in %.1fs — serving model params via NIXL", world_size, total_elapsed)

              # Keep all ranks alive (params must stay in GPU memory for RDMA)
              while True:
                  time.sleep(3600)
              PYEOF

              exec mpirun --allow-run-as-root -np ${TP_SIZE} python3 /tmp/source_tp8.py
          resources:
            limits:
              nvidia.com/gpu: "8"
              rdma/ib: "8"
            requests:
              nvidia.com/gpu: "8"
              rdma/ib: "8"
              memory: "200Gi"
              cpu: "16"
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
            - name: model-cache
              mountPath: /models
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 64Gi
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-weights-storage
      imagePullSecrets:
        - name: nvcr-imagepullsecret
