# SPDX-FileCopyrightText: Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# TRT-LLM Target Instance - HuggingFace mode for P2P transfer
# - Queries ModelExpress server for source metadata
# - Receives HuggingFace weights via NIXL RDMA
# - Saves in HF format for TRT-LLM PyTorch backend (no engine build needed!)
apiVersion: v1
kind: Service
metadata:
  name: trtllm-target
  labels:
    app: trtllm-target
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: 8000
      name: http
  selector:
    app: trtllm-target
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trtllm-target
  labels:
    app: trtllm-target
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trtllm-target
  template:
    metadata:
      labels:
        app: trtllm-target
    spec:
      containers:
        - name: trtllm-target
          # Use same working image as vLLM P2P (has NIXL 0.8.0 with working UCX)
          image: nvcr.io/nvidian/dynamo-dev/modelexpress-p2p-client:v0.1.0-baseline
          imagePullPolicy: Always
          # Needed for GPUDirect RDMA to work
          securityContext:
            capabilities:
              add:
                - IPC_LOCK
          env:
            # Model configuration - HuggingFace mode for DeepSeek-V3
            - name: MODEL_NAME
              value: "deepseek-v3"
            - name: OUTPUT_DIR
              value: "/tmp/mx_trtllm"
            - name: TP_SIZE
              value: "8"
            # ModelExpress server address
            - name: MODEL_EXPRESS_URL
              value: "modelexpress-server:8001"
            # Logging
            - name: NIXL_LOG_LEVEL
              value: "INFO"
            - name: UCX_LOG_LEVEL
              value: "WARN"
            # UCX transport settings for GPUDirect RDMA (same as vLLM)
            - name: UCX_TLS
              value: "rc_x,rc,dc_x,dc,cuda_copy"
            - name: UCX_RNDV_SCHEME
              value: "get_zcopy"
            - name: UCX_RNDV_THRESH
              value: "0"
            # Pod network info for NIXL
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -ex
              
              # Copy trtllm_loader.py from mounted ConfigMap to modelexpress package
              DEST="/usr/local/lib/python3.12/dist-packages/modelexpress"
              cp /opt/trtllm-code/trtllm_loader.py "${DEST}/trtllm_loader.py"
              
              # Update __init__.py to export trtllm classes
              echo "from .trtllm_loader import MxTrtllmSourcePublisher, MxTrtllmTargetLoader" >> "${DEST}/__init__.py"
              
              # Write Python script to file to avoid quote escaping issues
              cat > /tmp/run_target.py << 'PYTHON_SCRIPT'
              import os
              import logging
              
              logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')
              logger = logging.getLogger(__name__)
              
              from modelexpress import MxTrtllmTargetLoader
              
              model_name = os.environ.get("MODEL_NAME", "qwen-0.5b")
              mx_server = os.environ.get("MODEL_EXPRESS_URL", "modelexpress-server:8001")
              output_dir = os.environ.get("OUTPUT_DIR", "/tmp/mx_trtllm")
              tp_size = int(os.environ.get("TP_SIZE", "1"))
              
              logger.info("Starting TRT-LLM target loader (HuggingFace mode)")
              logger.info(f"  Model: {model_name}")
              logger.info(f"  MX Server: {mx_server}")
              logger.info(f"  Output: {output_dir}")
              logger.info(f"  TP Size: {tp_size}")
              
              loader = MxTrtllmTargetLoader(
                  model_name=model_name,
                  mx_server=mx_server,
                  output_dir=output_dir,
                  tp_size=tp_size,
              )
              
              logger.info("Receiving HuggingFace weights via P2P transfer...")
              # use_pytorch_backend=True for HuggingFace mode - no engine build needed!
              model_path = loader.load(use_pytorch_backend=True)
              
              stats = loader.get_transfer_stats()
              logger.info(f"Transfer complete: {stats.get('total_bytes', 0) / 1e9:.2f} GB")
              logger.info(f"Bandwidth: {stats.get('bandwidth_gbps', 0):.1f} Gbps")
              logger.info(f"Model saved to: {model_path}")
              logger.info("TRT-LLM target ready - use with: LLM(model=path, backend='pytorch')")
              
              import time
              while True:
                  time.sleep(3600)
              PYTHON_SCRIPT
              
              python3 /tmp/run_target.py
          resources:
            limits:
              nvidia.com/gpu: "8"
              rdma/ib: "8"
            requests:
              nvidia.com/gpu: "8"
              rdma/ib: "8"
              memory: "200Gi"
              cpu: "16"
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
            - name: tmp
              mountPath: /tmp
            - name: trtllm-code
              mountPath: /opt/trtllm-code

      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 64Gi
        - name: tmp
          emptyDir:
            sizeLimit: 500Gi
        - name: trtllm-code
          configMap:
            name: trtllm-loader-code
      imagePullSecrets:
        - name: nvcr-imagepullsecret
