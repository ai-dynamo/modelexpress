# SPDX-FileCopyrightText: Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# TRT-LLM Source Instance - HuggingFace mode for P2P transfer
# - Loads HuggingFace model directly to GPU (no checkpoint conversion needed!)
# - Registers with NIXL for RDMA access
# - Publishes metadata to ModelExpress server
# - Targets receive weights for TRT-LLM PyTorch backend (no engine build needed)
apiVersion: v1
kind: Service
metadata:
  name: trtllm-source
  labels:
    app: trtllm-source
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: 8000
      name: http
  selector:
    app: trtllm-source
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trtllm-source
  labels:
    app: trtllm-source
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trtllm-source
  template:
    metadata:
      labels:
        app: trtllm-source
    spec:
      containers:
        - name: trtllm-source
          # Use same working image as vLLM P2P (has NIXL 0.8.0 with working UCX)
          image: nvcr.io/nvidian/dynamo-dev/modelexpress-p2p-client:v0.1.0-baseline
          imagePullPolicy: Always
          # Needed for GPUDirect RDMA to work
          securityContext:
            capabilities:
              add:
                - IPC_LOCK
          env:
            # Model configuration - HuggingFace mode for Llama 70B Instruct
            - name: MODEL_NAME
              value: "llama-70b-instruct"
            - name: HF_MODEL_PATH
              value: "/models"
            - name: TP_SIZE
              value: "8"
            - name: DTYPE
              value: "bfloat16"
            # ModelExpress server address
            - name: MODEL_EXPRESS_URL
              value: "modelexpress-server:8001"
            # Logging
            - name: NIXL_LOG_LEVEL
              value: "INFO"
            - name: UCX_LOG_LEVEL
              value: "WARN"
            # UCX transport settings for GPUDirect RDMA (same as vLLM)
            - name: UCX_TLS
              value: "rc_x,rc,dc_x,dc,cuda_copy"
            - name: UCX_RNDV_SCHEME
              value: "get_zcopy"
            - name: UCX_RNDV_THRESH
              value: "0"
            # Pod network info for NIXL
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: HF_TOKEN
                  optional: true
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -ex
              
              # Copy trtllm_loader.py and updated p2p_pb2.py from ConfigMap
              DEST="/usr/local/lib/python3.12/dist-packages/modelexpress"
              cp /opt/trtllm-code/trtllm_loader.py "${DEST}/trtllm_loader.py"
              cp /opt/trtllm-code/p2p_pb2.py "${DEST}/p2p_pb2.py"
              
              # Update __init__.py to export trtllm classes
              echo "from .trtllm_loader import MxTrtllmSourcePublisher, MxTrtllmTargetLoader" >> "${DEST}/__init__.py"
              
              python3 -c "
              import os
              import time
              import logging
              
              logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')
              logger = logging.getLogger(__name__)
              
              # Import ModelExpress TRT-LLM loader
              from modelexpress import MxTrtllmSourcePublisher
              
              hf_model_path = os.environ.get('HF_MODEL_PATH', '/models/Qwen/Qwen2.5-0.5B')
              model_name = os.environ.get('MODEL_NAME', 'qwen-0.5b')
              mx_server = os.environ.get('MODEL_EXPRESS_URL', 'modelexpress-server:8001')
              tp_size = int(os.environ.get('TP_SIZE', '1'))
              dtype = os.environ.get('DTYPE', 'bfloat16')
              
              logger.info('Starting TRT-LLM source publisher (HuggingFace mode)')
              logger.info(f'  HF Model Path: {hf_model_path}')
              logger.info(f'  Model Name: {model_name}')
              logger.info(f'  MX Server: {mx_server}')
              logger.info(f'  TP Size: {tp_size}')
              logger.info(f'  Dtype: {dtype}')
              
              # Check model exists
              import os.path
              if not os.path.exists(hf_model_path):
                  logger.error(f'Model not found at {hf_model_path}')
                  logger.info('Available in /models:')
                  for f in os.listdir('/models'):
                      logger.info(f'  {f}')
                  raise FileNotFoundError(f'Model not found: {hf_model_path}')
              
              # Initialize publisher with HuggingFace mode
              publisher = MxTrtllmSourcePublisher(
                  model_name=model_name,
                  mx_server=mx_server,
                  hf_model_path=hf_model_path,
                  tp_size=tp_size,
                  dtype=dtype,
              )
              
              try:
                  publisher.initialize()
                  logger.info('Source publisher ready - serving weights via P2P')
                  
                  # Keep running to serve weights
                  while True:
                      time.sleep(3600)
                      
              except KeyboardInterrupt:
                  logger.info('Shutting down...')
              finally:
                  publisher.shutdown()
              "
          resources:
            limits:
              nvidia.com/gpu: "8"
              rdma/ib: "8"
            requests:
              nvidia.com/gpu: "8"
              rdma/ib: "8"
              memory: "200Gi"
              cpu: "16"
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
            - name: model-cache
              mountPath: /models
            - name: trtllm-code
              mountPath: /opt/trtllm-code

      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 64Gi
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-weights-storage
        - name: trtllm-code
          configMap:
            name: trtllm-loader-code
      imagePullSecrets:
        - name: nvcr-imagepullsecret
