# SPDX-FileCopyrightText: Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# TRT-LLM Source Instance - builds engine and publishes for P2P transfer
# - Builds TRT-LLM engine from checkpoint (one-time, cached)
# - Loads weights to GPU memory
# - Registers with NIXL for RDMA access
# - Publishes metadata to ModelExpress server
# - Targets receive weights and can run immediately (no build needed)
apiVersion: v1
kind: Service
metadata:
  name: trtllm-source
  labels:
    app: trtllm-source
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: 8000
      name: http
  selector:
    app: trtllm-source
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trtllm-source
  labels:
    app: trtllm-source
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trtllm-source
  template:
    metadata:
      labels:
        app: trtllm-source
    spec:
      containers:
        - name: trtllm-source
          image: nvcr.io/nvidia/tritonserver:24.12-trtllm-python-py3
          imagePullPolicy: Always
          # Needed for GPUDirect RDMA to work
          securityContext:
            capabilities:
              add:
                - IPC_LOCK
          env:
            # Model configuration
            - name: MODEL_NAME
              value: "qwen-0.5b"
            - name: CHECKPOINT_DIR
              value: "/models/qwen-0.5b/trtllm-checkpoint"
            - name: ENGINE_DIR
              value: "/models/qwen-0.5b/trtllm-engine"
            # Set to "1" to build engine on source (recommended)
            # Targets will receive engine + weights, no local build needed
            - name: BUILD_ENGINE
              value: "1"
            # ModelExpress server address
            - name: MODEL_EXPRESS_URL
              value: "modelexpress-server:8001"
            # Logging
            - name: NIXL_LOG_LEVEL
              value: "INFO"
            - name: UCX_LOG_LEVEL
              value: "WARN"
            # UCX transport settings for GPUDirect RDMA
            - name: UCX_TLS
              value: "rc_x,rc,dc_x,dc,cuda_copy"
            - name: UCX_RNDV_SCHEME
              value: "get_zcopy"
            - name: UCX_RNDV_THRESH
              value: "0"
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: HF_TOKEN
                  optional: true
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -ex
              
              # Install modelexpress package
              echo "Installing modelexpress..."
              pip install --quiet /opt/modelexpress/modelexpress_client/python/ || \
                pip install --quiet git+https://github.com/ai-dynamo/modelexpress.git#subdirectory=modelexpress_client/python
              
              # Install nixl for RDMA transfers
              pip install --quiet "nixl[cu12]" || echo "NIXL install failed, continuing..."
              
              python3 -c "
              import os
              import time
              import logging
              
              logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')
              logger = logging.getLogger(__name__)
              
              # Import ModelExpress TRT-LLM loader
              from modelexpress import MxTrtllmSourcePublisher
              
              checkpoint_dir = os.environ.get('CHECKPOINT_DIR', '/models/checkpoint')
              engine_dir = os.environ.get('ENGINE_DIR', '/models/engine')
              model_name = os.environ.get('MODEL_NAME', 'unknown')
              mx_server = os.environ.get('MODEL_EXPRESS_URL', 'modelexpress-server:8001')
              build_engine = os.environ.get('BUILD_ENGINE', '1') == '1'
              
              logger.info('Starting TRT-LLM source publisher')
              logger.info(f'  Checkpoint: {checkpoint_dir}')
              logger.info(f'  Engine: {engine_dir}')
              logger.info(f'  Model: {model_name}')
              logger.info(f'  MX Server: {mx_server}')
              logger.info(f'  Build engine: {build_engine}')
              
              # Check if engine already exists (skip rebuild)
              import os.path
              engine_exists = os.path.exists(engine_dir) and any(
                  f.endswith('.engine') for f in os.listdir(engine_dir)
              ) if os.path.exists(engine_dir) else False
              
              if engine_exists:
                  logger.info(f'Found existing engine at {engine_dir}')
              
              # Initialize publisher - builds engine if needed, then loads weights
              publisher = MxTrtllmSourcePublisher(
                  checkpoint_dir=checkpoint_dir,
                  model_name=model_name,
                  mx_server=mx_server,
                  engine_dir=engine_dir if build_engine else None,
              )
              
              try:
                  # build_if_needed=True will build engine if it doesn't exist
                  publisher.initialize(build_if_needed=build_engine)
                  logger.info('Source publisher ready - serving weights via P2P')
                  
                  # Keep running to serve weights
                  while True:
                      time.sleep(3600)
                      
              except KeyboardInterrupt:
                  logger.info('Shutting down...')
              finally:
                  publisher.shutdown()
              "
          resources:
            limits:
              nvidia.com/gpu: "1"
              rdma/ib: "1"
            requests:
              nvidia.com/gpu: "1"
              rdma/ib: "1"
              memory: "32Gi"
              cpu: "8"
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
            - name: model-cache
              mountPath: /models

      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 64Gi
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-weights-storage
      imagePullSecrets:
        - name: nvcr-imagepullsecret
