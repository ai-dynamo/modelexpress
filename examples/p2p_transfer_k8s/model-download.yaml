# SPDX-FileCopyrightText: Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# RWX PVC for shared model storage and Job to download HuggingFace model
#
# Usage:
#   1. Adjust storageClassName to match your cluster's RWX-capable storage class
#   2. kubectl apply -f model-download.yaml
#   3. Wait for job to complete: kubectl wait --for=condition=complete job/model-download --timeout=1h
#   4. Deploy vllm-source.yaml and vllm-target.yaml

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-cache
  labels:
    app: modelexpress
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: csi-mounted-fs-path-sc  # RWX-capable storage class
  resources:
    requests:
      storage: 1400Gi

---
apiVersion: batch/v1
kind: Job
metadata:
  name: model-download
  labels:
    app: modelexpress
spec:
  ttlSecondsAfterFinished: 3600  # Clean up 1 hour after completion
  template:
    spec:
      restartPolicy: OnFailure
      containers:
        - name: downloader
          image: python:3.11-slim
          env:
            - name: HF_HUB_CACHE
              value: /models
            - name: MODEL_NAME
              value: "deepseek-ai/DeepSeek-V3"
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: HF_TOKEN
          command: ["bash", "-c"]
          args:
            - |
              set -ex
              pip install huggingface_hub
              
              # Download model files (weights, config, tokenizer)
              python3 -c "
              import os
              from huggingface_hub import snapshot_download
              snapshot_download(
                  '${MODEL_NAME}',
                  token=os.environ.get('HF_TOKEN'),
              )
              print('Download complete')
              "
          resources:
            requests:
              memory: "8Gi"
              cpu: "4"
          volumeMounts:
            - name: model-cache
              mountPath: /models
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache
