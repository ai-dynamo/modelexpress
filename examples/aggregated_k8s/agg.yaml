# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-cache-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Gi
    limits:
      storage: 500Gi

---
apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: vllm-agg
spec:
  services:
    ModelExpress:
      envFromSecret: hf-token-secret
      livenessProbe:
        tcpSocket:
          port: 8000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      readinessProbe:
        tcpSocket:
          port: 8000
        initialDelaySeconds: 10
        periodSeconds: 5
        timeoutSeconds: 3
        failureThreshold: 3
      dynamoNamespace: vllm-agg
      componentType: frontend
      replicas: 1
      resources:
        requests:
          cpu: "4"
          memory: "16Gi"
        limits:
          cpu: "4"
          memory: "16Gi"
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/modelexpress-server:0.2.0
          imagePullPolicy: IfNotPresent
          env:
            - name: MODEL_EXPRESS_SERVER_PORT
              value: "8000"
            - name: MODEL_EXPRESS_LOGGING_LEVEL
              value: "info"
            - name: MODEL_EXPRESS_DATABASE_PATH
              value: "/root/models.db"
            - name: MODEL_EXPRESS_CACHE_DIRECTORY
              value: "/model/.model-express/cache"
            - name: MODEL_NAME
              value: "Qwen/Qwen3-0.6B"
            - name: MODEL_CACHE_PATH
              value: "/model/.model-express/cache"
            - name: HF_HUB_CACHE
              value: "/model/.model-express/cache"
          command:
            - /bin/sh
            - -c
          args:
            - |
              echo "Starting Model Express Server..."
              ./modelexpress-server &
              SERVER_PID=$!
              echo "Server started with PID: $SERVER_PID"

              echo "Setting up Model Express configuration..."
              mkdir -p $MODEL_CACHE_PATH
              mkdir -p /root/.model-express
              cat > /root/.model-express/config.yaml << EOF
              local_path: $MODEL_CACHE_PATH
              server_endpoint: http://localhost:8000
              timeout_secs: null
              EOF

              echo "Waiting for server to be ready..."
              for i in {1..60}; do
                if ./model-express-cli --endpoint http://localhost:8000 health > /dev/null 2>&1; then
                  echo "Server is ready!"
                  break
                fi
                echo "Waiting for server... ($i/60)"
                sleep 2
              done

              echo "Server PID: $SERVER_PID"
              wait $SERVER_PID
      pvcs:
        name: model-cache-pvc
        mountPoint: /root
    Frontend:
      dynamoNamespace: vllm-agg
      componentType: frontend
      replicas: 1
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:my-tag
      pvcs:
        name: model-cache-pvc
        mountPoint: /model
    VllmDecodeWorker:
      envFromSecret: hf-token-secret
      livenessProbe:
        httpGet:
          path: /live
          port: 9090
        periodSeconds: 5
        timeoutSeconds: 30
        failureThreshold: 1
      readinessProbe:
        httpGet:
          path: /health
          port: 9090
        periodSeconds: 10
        timeoutSeconds: 30
        failureThreshold: 60
      dynamoNamespace: vllm-agg
      componentType: worker
      replicas: 1
      resources:
        requests:
          cpu: "4"
          memory: "16Gi"
          gpu: "1"
        limits:
          cpu: "4"
          memory: "16Gi"
          gpu: "1"
      envs:
        - name: DYN_SYSTEM_ENABLED
          value: "true"
        - name: DYN_SYSTEM_USE_ENDPOINT_HEALTH_STATUS
          value: '["generate"]'
        - name: DYN_SYSTEM_PORT
          value: "9090"
        - name: MODEL_NAME
          value: "Qwen/Qwen3-0.6B"
        - name: MODEL_CACHE_PATH
          value: "/model/.model-express/cache"
        - name: HF_HUB_CACHE
          value: "/model/.model-express/cache"
        - name: MODEL_EXPRESS_URL
          value: "http://vllm-agg-modelexpress:8000"
        - name: VLLM_GPU_MEMORY_UTILIZATION
          value: "0.95"
        - name: VLLM_MAX_MODEL_LEN
          value: "8192"
      extraPodSpec:
        mainContainer:
          startupProbe:
            httpGet:
              path: /health
              port: 9090
            periodSeconds: 30
            failureThreshold: 240
          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:my-tag
          workingDir: /workspace/components/backends/vllm
          command:
            - /bin/sh
            - -c
          args:
            - |
              echo "=== VLLM Worker Startup Timing ==="
              START_TIME=$(date +%s)
              echo "Start time: $(date)"
              
              echo "Starting VLLM worker (model download will happen automatically)..."
              VLLM_START=$(date +%s)
              python3 -m dynamo.vllm --model $MODEL_NAME --tensor-parallel-size 1 | tee /tmp/vllm.log
          resources:
            requests:
              ephemeral-storage: "50Gi"
            limits:
              ephemeral-storage: "50Gi"
      pvcs:
        name: model-cache-pvc
        mountPoint: /model
